{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import GPT\n",
    "from gpt_config import GPTConfig\n",
    "import torch\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sentiment_classification.reviewsDataset import reviewsDataset\n",
    "from gpt_utils import dynamic_padding\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "def encode(s: str):\n",
    "    return enc.encode(s, allowed_special={\"<|endoftext|>\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rd = reviewsDataset(split=\"train\")\n",
    "# dl = DataLoader(rd,batch_size=2,collate_fn=dynamic_padding,shuffle=True)\n",
    "# for i in range(10):\n",
    "    # batch = next(iter(dl))\n",
    "    # print(batch[\"lengths\"])\n",
    "    # print((batch[\"attention_masks\"]))\n",
    "enc.decode([0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero shot evaluation\n",
    "from gpt import GPT\n",
    "from gpt_config import GPTConfig\n",
    "from gpt_utils import dynamic_padding\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "dl = DataLoader(rd,batch_size=2,collate_fn=dynamic_padding)\n",
    "batch = next(iter(dl))\n",
    "config = GPTConfig(use_lora=False)\n",
    "model = GPT.from_pretrained()\n",
    "model.crop_block_size(config.block_size)\n",
    "print(batch[\"input_ids\"].size())\n",
    "logits, _ = model(batch[\"input_ids\"],batch[\"attention_masks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import GPT\n",
    "from gpt_config import GPTConfig\n",
    "import loralib as lora\n",
    "import torch\n",
    "model_config = GPTConfig(use_lora=True,load_from_checkpoint=True,checkpoint_path=\"run/finetune_lora.ckpt\")\n",
    "model = GPT.from_pretrained(config=model_config)\n",
    "if model_config.use_lora:\n",
    "    lora.mark_only_lora_as_trainable(model)\n",
    "model_base = GPT.from_pretrained(config=GPTConfig(use_lora=False,block_size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(model_base))\n",
    "param_dict_base = {pn:p for pn,p in model_base.named_parameters()}\n",
    "param_dict = {pn:p for pn,p in model.named_parameters()}\n",
    "for k in param_dict_base.keys():\n",
    "    print(f\"{k}:{torch.equal(param_dict[k],param_dict_base[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "from gpt_karpathy import GPT as GPT_karpathy\n",
    "from gpt import GPT\n",
    "from gpt_config import GPTConfig\n",
    "\n",
    "model_base = GPT_karpathy.from_pretrained(\"gpt2\")\n",
    "model_base.crop_block_size(128)\n",
    "\n",
    "model = GPT.from_pretrained(config=GPTConfig(use_lora=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment_classification.reviewsDataset import reviewsDataset\n",
    "from gpt_utils import dynamic_padding\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch\n",
    "rd = reviewsDataset(split=\"train\")\n",
    "batch_rd = next(iter(rd))\n",
    "dl = DataLoader(rd,batch_size=2,collate_fn=dynamic_padding)\n",
    "batch = next(iter(dl))\n",
    "wte_k = model_base.transformer.wte(batch_rd['input_ids'])\n",
    "wte = model.transformer.wte(batch['input_ids'])\n",
    "input_len = sum(batch[\"attention_masks\"][0]).item()\n",
    "# print(wte.size())\n",
    "x_k = wte_k.unsqueeze(0)\n",
    "for block_k in model_base.transformer.h:\n",
    "    x_k = block_k(x_k)\n",
    "x = wte\n",
    "for block in model.transformer.h:\n",
    "    x = block(x,batch[\"attention_masks\"])\n",
    "x_k = model_base.transformer.ln_f(x_k)\n",
    "x_k = model_base.lm_head(x_k)\n",
    "x = model.transformer.ln_f(x)\n",
    "x = model.lm_head(x)\n",
    "\n",
    "# print(torch.equal(wte_k[:input_len],wte[0][:input_len]))\n",
    "# print(x[0][0:input_len],x_k.squeeze())\n",
    "# print(x.size(),x_k.size())\n",
    "# print(torch.isclose(x[0][0:input_len],x_k.squeeze(),atol=1e-4))\n",
    "# print(batch['input_ids'][0])\n",
    "# print(batch_rd[\"input_ids\"])\n",
    "# print(f\"Inputs Equal: {torch.equal(batch_rd['input_ids'], batch['input_ids'])}\")\n",
    "logits,_ = model_base(batch_rd[\"input_ids\"].view(1,-1))\n",
    "logits_x, _ = model(batch[\"input_ids\"],batch[\"attention_masks\"],batch[\"label_idxs\"])\n",
    "# print(logits)\n",
    "# print(\"---\")\n",
    "# print(logits_x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.equal(logits, logits_x))\n",
    "print(logits.squeeze().size(), logits_x[0].size())\n",
    "equality = torch.isclose(logits.squeeze(), logits_x[0],atol=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walk through a few examples of the data from encoding all the way to the attention output of the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data that the model is seeing.\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sentiment_classification.reviewsDataset import reviewsDataset\n",
    "from gpt_utils import dynamic_padding\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "def encode(s: str):\n",
    "    return enc.encode(s, allowed_special={\"<|endoftext|>\"},)\n",
    "\n",
    "rd = reviewsDataset(split=\"test\")\n",
    "batch_size = 1\n",
    "dl = DataLoader(rd,shuffle=True,batch_size=batch_size,collate_fn=dynamic_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for gpt2\n",
      "Number of parameters: 123.65M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/qmcgqwhj1mx925j0j_r36xp40000gn/T/ipykernel_87730/3642084628.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_config.checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model\n"
     ]
    }
   ],
   "source": [
    "# Load the finetuned model\n",
    "from gpt import GPT\n",
    "from gpt_config import GPTConfig\n",
    "\n",
    "model_config = GPTConfig(use_lora=True,load_from_checkpoint=True,checkpoint_path=\"run/dropout_low_lr/finetune_lora.ckpt\",debug=True)\n",
    "# model_config = GPTConfig(debug=True,binary_classification_head=False)\n",
    "model = GPT.from_pretrained(config=model_config)\n",
    "ckpt = torch.load(model_config.checkpoint_path)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()\n",
    "# device = \"mps\"\n",
    "print(\"Loaded Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: ASTROESQUE (2 outta 5 stars)I have no idea what the title is supposed... even less of an idea of what is supposed to be going on in this movie half the time... yet, it still kept me sort of interested. This is low, low budget film-making along the lines of \"El Mariachi\", filmed in 16mm... and, for what it's worth, the shots are very well-composed and visually stylish. Directed, written by and starring comic book writer/artist Michael Allred... I guess it's no surprise that the film Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/neg/5854_4.txt\n",
      "Label: 0.0\n",
      "Prediction: 0.20987161993980408\n",
      " stylish,0.926989734172821\n",
      " stylish,0.877033531665802\n",
      " surprise,0.8320409655570984\n",
      "red,0.8246546387672424\n",
      " movie,0.7579991817474365\n",
      " guess,0.717256486415863\n",
      " stylish,0.5912331938743591\n",
      " no,0.5118359327316284\n",
      " yet,0.4224642515182495\n",
      " low,0.3985898494720459\n",
      " low,0.34116581082344055\n",
      " low,0.24244755506515503\n",
      " Sent,0.2296426147222519\n",
      " low,0.21573254466056824\n",
      " low,0.20592588186264038\n",
      " supposed,0.149651437997818\n",
      " sort,0.1233498677611351\n",
      " stylish,0.1209011971950531\n",
      " Sent,0.07711176574230194\n",
      " comic,0.07273190468549728\n",
      "artist,0.06619568914175034\n",
      " budget,0.05582667142152786\n",
      "red,0.03339921683073044\n",
      "osed,0.02176997996866703\n",
      "--------------------------------------------\n",
      "Review: Very stark, very drab, no real drama. Why not just make a documentary? This isn't exactly The Passion of Joan of Arc. The only reason for seeing Chronicles is to hear the performances. I love Bach's music and even I found it hard to sit through this misery of a film. The great Gustav Leonhardt plays (in two senses of the word) Bach. We don't get much of a sense of him as an actor, since he's given so little to do dramatically. Mostly, he gets to walk purposefully or angrily out of various rooms. Bach's life, of course Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/neg/6814_3.txt\n",
      "Label: 0.0\n",
      "Prediction: 0.7533007264137268\n",
      " misery,0.9994657635688782\n",
      " angrily,0.7734280824661255\n",
      " isn,0.7731207013130188\n",
      " Joan,0.5784379839897156\n",
      " angrily,0.5559850931167603\n",
      " love,0.545230507850647\n",
      " misery,0.5089741349220276\n",
      " Sent,0.49537143111228943\n",
      " great,0.4923698306083679\n",
      " misery,0.49109914898872375\n",
      " dramatically,0.485935240983963\n",
      " Gustav,0.4608515799045563\n",
      " dramatically,0.4384743869304657\n",
      " Passion,0.38934507966041565\n",
      " drama,0.24751166999340057\n",
      " misery,0.24032463133335114\n",
      " misery,0.19367605447769165\n",
      " Passion,0.1650206744670868\n",
      " angrily,0.15882250666618347\n",
      " misery,0.11009056121110916\n",
      " sit,0.07088308781385422\n",
      " Gustav,0.05132610723376274\n",
      " misery,0.036876481026411057\n",
      " angrily,0.00041479041101410985\n",
      "--------------------------------------------\n",
      "Review: First off, this movie leaves you in a limbo mood wise. You don't know what to feel. So much so that you don't feel bad for Caines character when his son gets murdered (which was actually mostly due to bad editing). The script was too bland. None of the situations matter as you watch them. The soundtrack, or lack there of (if there was it wasn't good enough to even remember) does not help it one bit. Only good surprise to this movie was Andy Serkis' performance. It was on par if not better than Caine's. The story would Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/neg/6957_3.txt\n",
      "Label: 0.0\n",
      "Prediction: 0.02604794315993786\n",
      " bland,0.9994471669197083\n",
      " bland,0.9895148277282715\n",
      " bland,0.9265313148498535\n",
      " bad,0.8193469047546387\n",
      " bland,0.8120391964912415\n",
      " bad,0.6788623332977295\n",
      " bland,0.6495931148529053\n",
      "'t,0.5520970225334167\n",
      " bad,0.5006937384605408\n",
      " bad,0.4911087155342102\n",
      " movie,0.4400327801704407\n",
      " surprise,0.3483327329158783\n",
      " bad,0.27823173999786377\n",
      " performance,0.26598700881004333\n",
      " limbo,0.25757595896720886\n",
      " mostly,0.24579468369483948\n",
      " bad,0.22677212953567505\n",
      " good,0.1571226716041565\n",
      " bad,0.15621709823608398\n",
      "aine,0.12730933725833893\n",
      " does,0.08536877483129501\n",
      " bad,0.05497865006327629\n",
      " bad,0.005967563018202782\n",
      " help,0.0003661715309135616\n",
      "--------------------------------------------\n",
      "Review: Having endured this film last night, I turned off the DVD player with a sense of deserving a medal for having the stamina to see it through to the end. Throughout the film I felt that I was watching the storyline fillers that you get in a high budget porn movie. the acting was stiff and taut, camera work appalling, and the locations and sets were so poor it felt like they had borrowed them from the local High School \"Amateur Dramatic's Society\".The only saving grace for this movie was that it had Amy Adams and Harriet Sansom Harris in its credits, other than that it Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/neg/10631_1.txt\n",
      "Label: 0.0\n",
      "Prediction: 0.6332980990409851\n",
      " appalling,0.9995501637458801\n",
      " appalling,0.9976847171783447\n",
      " appalling,0.9945148229598999\n",
      " appalling,0.9873334169387817\n",
      " appalling,0.9705157279968262\n",
      " stiff,0.96079421043396\n",
      " locations,0.8792471885681152\n",
      " Harriet,0.8438883423805237\n",
      " appalling,0.6425780057907104\n",
      " poor,0.5644770860671997\n",
      " appalling,0.43526244163513184\n",
      " Sent,0.3066467046737671\n",
      " appalling,0.2831982970237732\n",
      " appalling,0.1816258579492569\n",
      " poor,0.1769908368587494\n",
      " poor,0.1044253557920456\n",
      " Sans,0.08458901941776276\n",
      " budget,0.0740942433476448\n",
      " appalling,0.019160939380526543\n",
      " Sans,0.011696085333824158\n",
      " poor,0.00720857921987772\n",
      "ateur,0.004149563144892454\n",
      " Harriet,0.0013466713717207313\n",
      " stiff,0.0002896892838180065\n",
      "--------------------------------------------\n",
      "Review: I've never read a good review for \"Vanity Fair\" and I can't understand why. For something that was \"rushed through in ten days\" it all comes off surprisingly well. Though admittedly \"Becky Sharp\" is a better movie and Miriam Hopkins a better Becky, there's nothing to stop this one from getting a solid 9/10. At times, Myrna Loy might seem just too cute and nice to be playing an utter bitch, but at other times she just has to squint her eyes and the air temperature drops a dozen degrees. Meow! The move to a Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/pos/7665_9.txt\n",
      "Label: 1.0\n",
      "Prediction: 0.5264268517494202\n",
      " nice,0.9642203450202942\n",
      " cute,0.9347174763679504\n",
      " cute,0.8939573764801025\n",
      " utter,0.7704465985298157\n",
      " nice,0.4981326758861542\n",
      " utter,0.47809866070747375\n",
      " nice,0.4746619164943695\n",
      " bitch,0.3421551287174225\n",
      " Sent,0.3419434726238251\n",
      " cute,0.2869870066642761\n",
      " cute,0.28390374779701233\n",
      " nice,0.28349366784095764\n",
      " cute,0.2791076898574829\n",
      " seem,0.27520787715911865\n",
      "na,0.270751953125\n",
      " nothing,0.20667673647403717\n",
      " better,0.20403750240802765\n",
      " utter,0.19771641492843628\n",
      " bitch,0.08632782846689224\n",
      " cute,0.0834699422121048\n",
      " degrees,0.08006221055984497\n",
      " nice,0.05950517952442169\n",
      " squ,0.05486450716853142\n",
      " cute,0.015290839597582817\n",
      "--------------------------------------------\n",
      "Review: I was very impressed with the latest production from Mick Molloy. As a fan of his, I was used to a different kind of humour than displayed here. He wisely opted with a more subtle, broad style of comedy in Crackerjack, rather than his usual low brow, in-your-face ramblings. It is, at times, inconsistent and un-even, but a decent script works past that, and makes for some entertaining viewing. Directed by Paul Moloney (who has directed almost every Australian TV series imaginable), Crackerjack tells the story of Jack Simpson, a Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/pos/11708_7.txt\n",
      "Label: 1.0\n",
      "Prediction: 0.9276859164237976\n",
      " inconsistent,0.9830294251441956\n",
      " inconsistent,0.9035001993179321\n",
      " Mol,0.8082797527313232\n",
      " entertaining,0.7447307705879211\n",
      " entertaining,0.6058788895606995\n",
      " entertaining,0.5873130559921265\n",
      " decent,0.4142346680164337\n",
      " inconsistent,0.4032805562019348\n",
      " Mol,0.38777098059654236\n",
      " opted,0.24501897394657135\n",
      " subtle,0.23684394359588623\n",
      " fan,0.22598622739315033\n",
      " Direct,0.22194264829158783\n",
      " imaginable,0.22057925164699554\n",
      " inconsistent,0.21747329831123352\n",
      " imaginable,0.18555951118469238\n",
      " imaginable,0.17081093788146973\n",
      " script,0.15775251388549805\n",
      " decent,0.15383872389793396\n",
      " decent,0.14264141023159027\n",
      " style,0.12411024421453476\n",
      " inconsistent,0.07382429391145706\n",
      " decent,0.06369336694478989\n",
      "jack,0.010920283384621143\n",
      "--------------------------------------------\n",
      "Review: The Frozen Limits is a big screen vehicle for the artists known as The Crazy Gang. They were a group of British entertainers who formed in the early 1930s. In the main the group's six men were Bud Flanagan, Chesney Allen, Jimmy Nervo, Teddy Knox, Charlie Naughton and Jimmy Gold. Hugely popular in the variety halls the group were also darlings of the then Royal Family. The plot here sees them as the Wonder Boys troupe who set off to seek their fortunes in Alaska after reading about a gold rush in the newspaper. Only problem is is that Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/pos/12162_8.txt\n",
      "Label: 1.0\n",
      "Prediction: 0.9599161148071289\n",
      " Crazy,0.9635748267173767\n",
      "erv,0.8707485795021057\n",
      " popular,0.616524875164032\n",
      " trou,0.5515931844711304\n",
      " Crazy,0.48632505536079407\n",
      " Sent,0.4628896713256836\n",
      " fortunes,0.4309117794036865\n",
      " 1930,0.42787113785743713\n",
      " Crazy,0.34720540046691895\n",
      " British,0.34058693051338196\n",
      " Teddy,0.29833051562309265\n",
      " here,0.27404114603996277\n",
      " Crazy,0.27067795395851135\n",
      " here,0.23885087668895721\n",
      " Knox,0.2304549515247345\n",
      " Huge,0.2147626280784607\n",
      " rush,0.18107172846794128\n",
      " problem,0.1552436351776123\n",
      " variety,0.15130957961082458\n",
      " artists,0.14062263071537018\n",
      " newspaper,0.1364421546459198\n",
      " entertain,0.12657561898231506\n",
      " fortunes,0.06595079600811005\n",
      " Huge,0.015711551532149315\n",
      "--------------------------------------------\n",
      "Review: Dear SciFi Channel: How have you been? How was your summer? I've been OK, but I feel like our relationship isn't the same anymore and we're growing apart. I don't understand why you don't love me anymore. I've just finished watching your SciFi Channel Original \"Skeleton Man\" and, once again, you've shown a blatant lack of respect for my feelings by KILLING OFF EVERY HOT GIRL IN THIS MOVIE!!! I mean, I understand that you're just in this for instant gratification. All you care about is producing a movie where people get sliced Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/neg/5392_4.txt\n",
      "Label: 0.0\n",
      "Prediction: 0.5872263312339783\n",
      " HOT,0.9977869987487793\n",
      " blatant,0.9906232953071594\n",
      " love,0.7976087331771851\n",
      " blatant,0.7303061485290527\n",
      " HOT,0.672389030456543\n",
      " care,0.6328968405723572\n",
      " gratification,0.5582589507102966\n",
      " don,0.4082387387752533\n",
      " Sent,0.37193167209625244\n",
      " blatant,0.32720720767974854\n",
      " OFF,0.3182907998561859\n",
      " HOT,0.24168388545513153\n",
      " blatant,0.21650578081607819\n",
      "keleton,0.2077471762895584\n",
      " just,0.1740698218345642\n",
      " sliced,0.15780338644981384\n",
      " blatant,0.14293600618839264\n",
      " blatant,0.140558123588562\n",
      " THIS,0.13485945761203766\n",
      " respect,0.13320167362689972\n",
      " respect,0.09863097220659256\n",
      " HOT,0.09213476628065109\n",
      " gratification,0.0036344178952276707\n",
      " KILL,0.001010132604278624\n",
      "--------------------------------------------\n",
      "Review: I would give this television series a 10 plus if i could. The writers were \"smack on\" and I think the best actors and actresses were a bonus to the show.These characters were so real. One could tell that from the two main actresses Ms. Toussaint & Ms. Potts that their relationship on & off camera was genuine It didn't just end when you hear those familiar words \"Cut\" at the end of a \"Take\". The show has thought me a lot about life for e.g. Historical struggles, tragedies and triumphs,relationships,every day situations Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/pos/3004_10.txt\n",
      "Label: 1.0\n",
      "Prediction: 0.8930282592773438\n",
      " didn,0.9666374325752258\n",
      " familiar,0.6108765602111816\n",
      " Sent,0.5184100270271301\n",
      " struggles,0.4804180860519409\n",
      " end,0.4802074730396271\n",
      " actresses,0.4245797395706177\n",
      "ous,0.4019638001918793\n",
      " end,0.38347965478897095\n",
      " end,0.33911123871803284\n",
      " end,0.33441081643104553\n",
      " Historical,0.28132686018943787\n",
      " end,0.2802352011203766\n",
      "g,0.2710423171520233\n",
      "g,0.2692604660987854\n",
      " tragedies,0.24413444101810455\n",
      " bonus,0.2430134415626526\n",
      " end,0.22039169073104858\n",
      " T,0.20082764327526093\n",
      " actresses,0.18426655232906342\n",
      " genuine,0.16911886632442474\n",
      " life,0.13662296533584595\n",
      " tragedies,0.13124307990074158\n",
      " One,0.06186066195368767\n",
      " lot,0.00332199246622622\n",
      "--------------------------------------------\n",
      "Review: Basil Rathbone and Nigel Bruce return as Sherlock Holmes and Dr. Watson in this superior tale of Holmes battling the 3rd Reich and the mastermind genius of Professor Moriarty. The film opens up in Switzerland as Holmes is in disguise as an \"old bookseller.\" He must bring Dr. Tobel and the Tobel Bomb Site to England before the Germans can kidnap Tobel. Holmes succeeds and the Germans recuit the evil Professor. Moriarty manages to outwit Scotland yard and LeStrad \"Dennis Hoey\" by kidnapping Tobel. The only clue left by Tobel is a list Sentiment:\n",
      "/Users/varun/Downloads/aclImdb/test/pos/9424_9.txt\n",
      "Label: 1.0\n",
      "Prediction: 0.7988787293434143\n",
      " evil,0.9998291730880737\n",
      " superior,0.9976558685302734\n",
      " list,0.7313815951347351\n",
      " evil,0.7213597297668457\n",
      " evil,0.6874063611030579\n",
      " evil,0.6302179098129272\n",
      " evil,0.5749086141586304\n",
      " Mori,0.5257904529571533\n",
      " genius,0.3750271797180176\n",
      " Sent,0.3439420163631439\n",
      " H,0.31531327962875366\n",
      " Mori,0.2694150507450104\n",
      " superior,0.2459464818239212\n",
      " Tob,0.2391812801361084\n",
      "el,0.2179284393787384\n",
      " Tob,0.19399254024028778\n",
      "el,0.14514176547527313\n",
      " genius,0.09734150022268295\n",
      " evil,0.0969787985086441\n",
      " succeeds,0.09209316223859787\n",
      "arty,0.09147472679615021\n",
      " Sent,0.07871674746274948\n",
      " genius,0.0014217739226296544\n",
      " Reich,0.0001183611384476535\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1336)\n",
    "pos_neg = rd.get_pos_neg_indices()\n",
    "for i in range(10):\n",
    "    batch = next(iter(dl))\n",
    "    logits, _,att_out = model(batch[\"input_ids\"],batch[\"attention_masks\"])\n",
    "    \n",
    "    predictions = F.sigmoid(logits)\n",
    "    # For each attention head, pick the top 10 indices\n",
    "    att_out = att_out.squeeze()\n",
    "    topk_vals = []\n",
    "    topk_indices = []\n",
    "    for head in range(att_out.shape[0]):\n",
    "        topk_v,topk_idx = torch.topk(att_out[head,:],2)\n",
    "        topk_vals.append(topk_v)\n",
    "        topk_indices.append(topk_idx)\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        valid = batch[\"attention_masks\"][j].sum()\n",
    "        review = enc.decode(batch['input_ids'][j][0:valid].tolist())\n",
    "        # if logits[j][pos_neg[\"positive\"]] > logits[j][pos_neg[\"negative\"]]:\n",
    "        #     prediction = 1\n",
    "        # else:\n",
    "        #     prediction = 0 \n",
    "        print(review)\n",
    "        print(f\"{batch['fpaths'][j]}\")\n",
    "        print(f\"Label: {batch['labels'][j]}\")\n",
    "        # print(f\"Prediction: {prediction}\")\n",
    "        print(f\"Prediction: {predictions[j].item()}\")\n",
    "        words = []\n",
    "        for head in range(att_out.shape[0]):\n",
    "            for val,idx in zip(topk_vals[head],topk_indices[head]):\n",
    "                words.append((enc.decode([batch['input_ids'][j][idx].item()]),val.item()))\n",
    "                # print(f\"Word: {enc.decode([batch['input_ids'][j][idx].item()])}, Attention Val: {val}\")\n",
    "        words = sorted(words,key=lambda x: -x[1])\n",
    "        for w,c in words:\n",
    "            print(f\"{w},{c}\")\n",
    "    \n",
    "    print(\"--------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
