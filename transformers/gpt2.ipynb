{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 for Sentiment Analysis on IMDb movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](##Introduction)\n",
    "2. [Data exploration](##Data-Exploration)\n",
    "3. [Zero Shot Classification](##Zero-shot-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The [IMDb](https://ai.stanford.edu/~amaas/data/sentiment/) is a binary sentiment classification dataset consisting of 100k movie reviews(50k positive and 50k negative). The dataset is split into train and test containing 50k reviews each.\n",
    "\n",
    "In this notebook, my goals are:\n",
    "1. Understand and implement [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Run GPT-2 on the IMDb classification task.\n",
    "2. Fine-tune GPT-2 for sentiment classification in under ~30 minutes on a 8GB Apple M2 macbook air (Faster if you have a Nvidia GPU).\n",
    "3. Understand how [LoRA](https://arxiv.org/abs/2106.09685) is implemented and use it to fine-tune GPT-2 for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Exploration\n",
    "Get a summary of the dataset. i.e\n",
    "1. No of samples\n",
    "2. No of positive / negative samples.\n",
    "3. Length of the movie reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from torch.utils.data import Dataset\n",
    "from sentiment_classification.reviewsDataset import reviewsDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploration\n",
    "\n",
    "imdb_train = reviewsDataset(\"train\",max_length=10000)\n",
    "imdb_test = reviewsDataset(\"test\",max_length=10000)\n",
    "\n",
    "\n",
    "def format_data(dataset: Dataset) -> pandas.DataFrame:\n",
    "\n",
    "    data = []\n",
    "    for batch in dataset:\n",
    "        data.append({\"input_ids\":len(batch[\"input_ids\"]),\n",
    "                    \"label\": batch[\"label\"],\n",
    "                    \"filename\": batch[\"fpath\"]})\n",
    "    \n",
    "    return pandas.DataFrame(data)\n",
    "\n",
    "train_data = format_data(imdb_train)\n",
    "test_data = format_data(imdb_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary statistics of the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(data: pandas.DataFrame) -> None:\n",
    "    print(f\"Number of reviews: {len(data)}\")\n",
    "    print(f\"Positive Reviews: {data[data['label'] == 1]['label'].count()}\")\n",
    "    print(f\"Negative Reviews: {data[data['label'] == 0]['label'].count()}\")\n",
    "    print(f\"Max Review Length: {data['input_ids'].max()}\\nMin Review Length: {data['input_ids'].min()}\")\n",
    "    print(f\"Median Review Length: {data['input_ids'].median()}\\nMean Review Length: {data['input_ids'].mean()}\")\n",
    "\n",
    "print(\"Train\\n--------------\")\n",
    "summary(train_data)\n",
    "print(\"Test\\n---------------\")\n",
    "summary(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Length of reviews (measured by the number of tokens)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_hist(title: str,df: pandas.DataFrame) -> None:\n",
    "    plt.figure()\n",
    "    plt.hist(df[\"input_ids\"],bins=100)\n",
    "    plt.xlabel(f\"No of tokens\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"{title}\")\n",
    "\n",
    "plot_hist(title='Train Data', df=train_data) \n",
    "plot_hist(title=\"Test Data\", df=test_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(title=\"Positive Reviews Test\",df=test_data[test_data['label']==1])\n",
    "plot_hist(title=\"Negative Reviews Test\",df=test_data[test_data['label']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test.py in `sentiment_classification` and write the results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_by_bin(results, bins,threshold=0.5):\n",
    "    TP = len(results[(results[\"label\"] >= threshold) & (results[\"prediction\"] >= threshold)])\n",
    "    FP = len(results[(results[\"label\"] < threshold) & (results[\"prediction\"] >= threshold)])\n",
    "    TN = len(results[(results[\"label\"] < threshold) & (results[\"prediction\"] < threshold)])\n",
    "    FN = len(results[(results[\"label\"] > threshold) & (results[\"prediction\"] < threshold)])\n",
    "    \n",
    "    print(\"Metrics\")\n",
    "    print(f\"Precision: {TP/(TP+FP)}\\nRecall: {TP/(TP+FN)}\\nAccuracy: {(TP+TN)/len(results)}\")\n",
    "    bins = range(0,1500,128)\n",
    "    results[\"bin\"] = pandas.cut(results['length'],bins)\n",
    "    metrics_by_bin = results.groupby('bin').apply(lambda x: pandas.Series({\"TP\": ((x[\"label\"] >= threshold) & (x[\"prediction\"] >= threshold)).sum(),\n",
    "                                                                            \"FP\":((x[\"label\"] < threshold) & (x[\"prediction\"] >= threshold)).sum(),\n",
    "                                                                            \"FN\": ((x[\"label\"] >= threshold) & (x[\"prediction\"] < threshold)).sum(),\n",
    "                                                                            \"TN\": ((x[\"label\"] < threshold) & (x[\"prediction\"] < threshold)).sum()}))\n",
    "\n",
    "    metrics_by_bin[\"accuracy\"] = (metrics_by_bin[\"TP\"] + metrics_by_bin[\"TN\"])/(metrics_by_bin[\"TP\"] + metrics_by_bin[\"TN\"]+ metrics_by_bin[\"FP\"]+ metrics_by_bin[\"FN\"])\n",
    "    metrics_by_bin[\"precision\"] = metrics_by_bin[\"TP\"]/(metrics_by_bin[\"TP\"] + metrics_by_bin[\"FP\"])\n",
    "    metrics_by_bin[\"recall\"] = metrics_by_bin[\"TP\"]/(metrics_by_bin[\"TP\"] + metrics_by_bin[\"FN\"])\n",
    "    print(\"Metrics by bin\")\n",
    "    print(metrics_by_bin.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the next word given the following prompt\n",
    " \n",
    "'''\n",
    "Review: The movie was awesome. Sentiment: Positive. \n",
    "Review: The performances were disappointing. Sentiment: Negative. \n",
    "Review: {review} Sentiment:\n",
    "'''\n",
    "I calculate the probabilities of the word \" Positive\" and \" Negative\" and classify the review based on which probability is greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run evaluation for the zero shot approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentiment_classification.reviewsDataset import reviewsDataset\n",
    "from sentiment_classification.eval import Eval\n",
    "from sentiment_classification.eval_config import EvalConfig\n",
    "from gpt_config import GPTConfig\n",
    "\n",
    "torch.manual_seed(1336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPTConfig(block_size=128,use_lora=False)\n",
    "eval_config = EvalConfig(results_path=\"zero_shot_128.txt\",subset=False)\n",
    "test_set = reviewsDataset(split=\"test\")\n",
    "evaluator = Eval(test_set=test_set,eval_config=eval_config,model_config=model_config)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = pandas.read_csv(\"zero_shot_128.txt\")\n",
    "bins = range(0,1500,128)\n",
    "get_metrics_by_bin(res_file,bins,threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finetuning without LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentiment_classification.train import Trainer\n",
    "from sentiment_classification.train_config import TrainConfig\n",
    "from gpt_config import GPTConfig\n",
    "from sentiment_classification.reviewsDataset import reviewsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(out_dir=\"run\",init_from=\"resume\",checkpoint_name=\"finetune_no_lora.ckpt\")\n",
    "model_config = GPTConfig(use_lora=False)\n",
    "rd = reviewsDataset(split=\"train\",max_length=model_config.block_size)\n",
    "train_set, val_set = torch.utils.data.random_split(rd,[0.85,0.15])\n",
    "trainer = Trainer(train_set,val_set,train_config,model_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run eval using the fine-tuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPTConfig(block_size=128,use_lora=False,load_from_checkpoint=True,checkpoint_path=\"run/finetune_no_lora.ckpt\")\n",
    "eval_config = EvalConfig(results_path=\"finetuned_no_lora.txt\",subset=False)\n",
    "test_set = reviewsDataset(split=\"test\")\n",
    "evaluator = Eval(test_set=test_set,eval_config=eval_config,model_config=model_config)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the performance of the fine-tuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = pandas.read_csv(\"finetuned_no_lora.txt\")\n",
    "bins = range(0,1500,128)\n",
    "get_metrics_by_bin(res_file,bins,threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run training using LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(out_dir=\"run\",checkpoint_name=\"finetune_lora.ckpt\",always_save_checkpoint=False,max_iters=2000)\n",
    "model_config = GPTConfig(block_size=128,use_lora=True,r=8)\n",
    "rd = reviewsDataset(split=\"train\",max_length=model_config.block_size)\n",
    "train_set, val_set = torch.utils.data.random_split(rd,[0.85,0.15])\n",
    "trainer = Trainer(train_set,val_set,train_config,model_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate using the LoRA finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPTConfig(use_lora=True,load_from_checkpoint=True,checkpoint_path=\"run/finetune_lora.ckpt\")\n",
    "eval_config = EvalConfig(results_path=\"finetuned_lora.txt\")\n",
    "test_set = reviewsDataset(split=\"test\")\n",
    "evaluator = Eval(test_set=test_set,eval_config=eval_config,model_config=model_config)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the performance of the LoRA finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = pandas.read_csv(\"finetuned_lora_256.txt\")\n",
    "bins = range(0,1500,128)\n",
    "get_metrics_by_bin(res_file,bins,threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
