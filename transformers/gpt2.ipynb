{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 for Sentiment Analysis on IMDb movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](##Introduction)\n",
    "2. [Data exploration](##Data-Exploration)\n",
    "3. [Zero Shot Classification](##Zero-shot-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The [IMDb](https://ai.stanford.edu/~amaas/data/sentiment/) is a binary sentiment classification dataset consisting of 100k movie reviews(50k positive and 50k negative). The dataset is split into train and test containing 50k reviews each.\n",
    "\n",
    "In this notebook, my goals are:\n",
    "1. Understand and implement [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Run GPT-2 on the IMDb classification task.\n",
    "2. Fine-tune GPT-2 for sentiment classification in under ~30 minutes on a 8GB Apple M2 macbook air (Faster if you have a Nvidia GPU).\n",
    "3. Understand how [LoRA](https://arxiv.org/abs/2106.09685) is implemented and use it to fine-tune GPT-2 for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Exploration\n",
    "Get a summary of the dataset. i.e\n",
    "1. No of samples\n",
    "2. No of positive / negative samples.\n",
    "3. Length of the movie reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from gpt_config import GPTConfig\n",
    "from sentiment_classification.reviewsDataset import reviewsDataset\n",
    "from sentiment_classification.eval import Eval\n",
    "from sentiment_classification.eval_config import EvalConfig\n",
    "from sentiment_classification.train import Trainer\n",
    "from sentiment_classification.train_config import TrainConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploration\n",
    "\n",
    "imdb_train = reviewsDataset(\"train\",max_length=10000)\n",
    "imdb_test = reviewsDataset(\"test\",max_length=10000)\n",
    "\n",
    "\n",
    "def format_data(dataset: Dataset) -> pandas.DataFrame:\n",
    "\n",
    "    data = []\n",
    "    for batch in dataset:\n",
    "        data.append({\"input_ids\":len(batch[\"input_ids\"]),\n",
    "                    \"label\": batch[\"label\"],\n",
    "                    \"filename\": batch[\"fpath\"]})\n",
    "    \n",
    "    return pandas.DataFrame(data)\n",
    "\n",
    "train_data = format_data(imdb_train)\n",
    "test_data = format_data(imdb_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary statistics of the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(data: pandas.DataFrame) -> None:\n",
    "    print(f\"Number of reviews: {len(data)}\")\n",
    "    print(f\"Positive Reviews: {data[data['label'] == 1]['label'].count()}\")\n",
    "    print(f\"Negative Reviews: {data[data['label'] == 0]['label'].count()}\")\n",
    "    print(f\"Max Review Length: {data['input_ids'].max()}\\nMin Review Length: {data['input_ids'].min()}\")\n",
    "    print(f\"Median Review Length: {data['input_ids'].median()}\\nMean Review Length: {data['input_ids'].mean()}\")\n",
    "\n",
    "print(\"Train\\n--------------\")\n",
    "summary(train_data)\n",
    "print(\"Test\\n---------------\")\n",
    "summary(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Length of reviews (measured by the number of tokens)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_hist(title: str,df: pandas.DataFrame) -> None:\n",
    "    plt.figure()\n",
    "    plt.hist(df[\"input_ids\"],bins=100)\n",
    "    plt.xlabel(f\"No of tokens\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"{title}\")\n",
    "\n",
    "plot_hist(title='Train Data', df=train_data) \n",
    "plot_hist(title=\"Test Data\", df=test_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(title=\"Positive Reviews Test\",df=test_data[test_data['label']==1])\n",
    "plot_hist(title=\"Negative Reviews Test\",df=test_data[test_data['label']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test.py in `sentiment_classification` and write the results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_by_bin(results, bins,threshold=0.5):\n",
    "    TP = len(results[(results[\"label\"] >= threshold) & (results[\"prediction\"] >= threshold)])\n",
    "    FP = len(results[(results[\"label\"] < threshold) & (results[\"prediction\"] >= threshold)])\n",
    "    TN = len(results[(results[\"label\"] < threshold) & (results[\"prediction\"] < threshold)])\n",
    "    FN = len(results[(results[\"label\"] > threshold) & (results[\"prediction\"] < threshold)])\n",
    "    \n",
    "    print(\"Metrics\")\n",
    "    print(f\"Precision: {TP/(TP+FP)}\\nRecall: {TP/(TP+FN)}\\nAccuracy: {(TP+TN)/len(results)}\")\n",
    "    bins = range(0,1500,128)\n",
    "    results[\"bin\"] = pandas.cut(results['length'],bins)\n",
    "    metrics_by_bin = results.groupby('bin').apply(lambda x: pandas.Series({\"TP\": ((x[\"label\"] >= threshold) & (x[\"prediction\"] >= threshold)).sum(),\n",
    "                                                                            \"FP\":((x[\"label\"] < threshold) & (x[\"prediction\"] >= threshold)).sum(),\n",
    "                                                                            \"FN\": ((x[\"label\"] >= threshold) & (x[\"prediction\"] < threshold)).sum(),\n",
    "                                                                            \"TN\": ((x[\"label\"] < threshold) & (x[\"prediction\"] < threshold)).sum()}))\n",
    "\n",
    "    metrics_by_bin[\"accuracy\"] = (metrics_by_bin[\"TP\"] + metrics_by_bin[\"TN\"])/(metrics_by_bin[\"TP\"] + metrics_by_bin[\"TN\"]+ metrics_by_bin[\"FP\"]+ metrics_by_bin[\"FN\"])\n",
    "    metrics_by_bin[\"precision\"] = metrics_by_bin[\"TP\"]/(metrics_by_bin[\"TP\"] + metrics_by_bin[\"FP\"])\n",
    "    metrics_by_bin[\"recall\"] = metrics_by_bin[\"TP\"]/(metrics_by_bin[\"TP\"] + metrics_by_bin[\"FN\"])\n",
    "    print(\"Metrics by bin\")\n",
    "    print(metrics_by_bin.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the next word given the following prompt\n",
    " \n",
    "'''\n",
    "Review: The movie was awesome. Sentiment: Positive. \n",
    "Review: The performances were disappointing. Sentiment: Negative. \n",
    "Review: {review} Sentiment:\n",
    "'''\n",
    "I calculate the probabilities of the word \" Positive\" and \" Negative\" and classify the review based on which probability is greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run evaluation for the zero shot approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPTConfig(block_size=128,use_lora=False)\n",
    "eval_config = EvalConfig(results_path=\"zero_shot_128.txt\",subset=False)\n",
    "test_set = reviewsDataset(split=\"test\")\n",
    "evaluator = Eval(test_set=test_set,eval_config=eval_config,model_config=model_config)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = pandas.read_csv(\"zero_shot_128.txt\")\n",
    "bins = range(0,1500,128)\n",
    "get_metrics_by_bin(res_file,bins,threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finetuning without LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(out_dir=\"run/first_part_review_no_lora/\",init_from=\"gpt2\",checkpoint_name=\"finetune_no_lora.ckpt\")\n",
    "model_config = GPTConfig(use_lora=False)\n",
    "rd = reviewsDataset(split=\"train\",max_length=model_config.block_size)\n",
    "train_set, val_set = torch.utils.data.random_split(rd,[0.85,0.15])\n",
    "trainer = Trainer(train_set,val_set,train_config,model_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run eval using the fine-tuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPTConfig(block_size=128,use_lora=False,load_from_checkpoint=True,checkpoint_path=\"run/finetune_no_lora.ckpt\")\n",
    "eval_config = EvalConfig(results_path=\"finetuned_no_lora.txt\",subset=False)\n",
    "test_set = reviewsDataset(split=\"test\")\n",
    "evaluator = Eval(test_set=test_set,eval_config=eval_config,model_config=model_config)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the performance of the fine-tuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file = pandas.read_csv(\"finetuned_no_lora.txt\")\n",
    "bins = range(0,1500,128)\n",
    "get_metrics_by_bin(res_file,bins,threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run training using LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for gpt2\n",
      "Number of parameters: 123.65M\n",
      "Resuming training from run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varun/Documents/learning/Projects/ml-practice/transformers/sentiment_classification/train.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.ckpt = torch.load(ckpt_path,map_location=self.train_config.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 25, with 295,680 parameters\n",
      "num non-decayed parameter tensors: 0, with 0 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8000\n",
      " Train Loss: 0.5172512531280518\n",
      "Validation Loss: 0.5576334595680237\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2000/52000 [07:09<2:42:08,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10000\n",
      " Train Loss: 0.41479671001434326\n",
      "Validation Loss: 0.4026747941970825\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4000/52000 [14:14<2:49:27,  4.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 12000\n",
      " Train Loss: 0.48558348417282104\n",
      "Validation Loss: 0.4565548300743103\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6000/52000 [21:21<2:35:48,  4.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 14000\n",
      " Train Loss: 0.47223418951034546\n",
      "Validation Loss: 0.41489166021347046\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 8000/52000 [28:32<2:34:47,  4.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 16000\n",
      " Train Loss: 0.35626357793807983\n",
      "Validation Loss: 0.4491492509841919\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 10000/52000 [35:37<2:31:28,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 18000\n",
      " Train Loss: 0.4400196373462677\n",
      "Validation Loss: 0.3557663857936859\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 12000/52000 [42:47<2:14:25,  4.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20000\n",
      " Train Loss: 0.40503746271133423\n",
      "Validation Loss: 0.3974704444408417\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 14000/52000 [49:58<2:12:03,  4.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 22000\n",
      " Train Loss: 0.35306239128112793\n",
      "Validation Loss: 0.48951295018196106\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 16000/52000 [57:10<1:54:25,  5.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 24000\n",
      " Train Loss: 0.4627331495285034\n",
      "Validation Loss: 0.4639752209186554\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 18000/52000 [1:04:21<2:02:55,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 26000\n",
      " Train Loss: 0.4530695080757141\n",
      "Validation Loss: 0.40638023614883423\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 20000/52000 [1:11:36<1:48:04,  4.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 28000\n",
      " Train Loss: 0.36468324065208435\n",
      "Validation Loss: 0.5254888534545898\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 22000/52000 [1:18:48<1:45:40,  4.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 30000\n",
      " Train Loss: 0.31557270884513855\n",
      "Validation Loss: 0.35610634088516235\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 24000/52000 [1:26:00<1:42:10,  4.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 32000\n",
      " Train Loss: 0.3332388997077942\n",
      "Validation Loss: 0.4747641086578369\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 26000/52000 [1:33:12<1:35:10,  4.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 34000\n",
      " Train Loss: 0.38842177391052246\n",
      "Validation Loss: 0.34253016114234924\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 28000/52000 [1:40:23<1:07:21,  5.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 36000\n",
      " Train Loss: 0.33774372935295105\n",
      "Validation Loss: 0.5448333024978638\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 30000/52000 [1:47:35<1:22:49,  4.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 38000\n",
      " Train Loss: 0.26978909969329834\n",
      "Validation Loss: 0.450605571269989\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 32000/52000 [1:54:41<1:05:12,  5.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 40000\n",
      " Train Loss: 0.3024047911167145\n",
      "Validation Loss: 0.39471179246902466\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 34000/52000 [2:01:49<1:05:28,  4.58it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 42000\n",
      " Train Loss: 0.21444904804229736\n",
      "Validation Loss: 0.339535117149353\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 36000/52000 [2:08:54<55:24,  4.81it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 44000\n",
      " Train Loss: 0.3342539966106415\n",
      "Validation Loss: 0.4235295355319977\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 38000/52000 [2:16:05<52:29,  4.45it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 46000\n",
      " Train Loss: 0.33049532771110535\n",
      "Validation Loss: 0.3862728774547577\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 40000/52000 [2:23:22<40:18,  4.96it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 48000\n",
      " Train Loss: 0.3497768044471741\n",
      "Validation Loss: 0.49080517888069153\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 42000/52000 [2:30:36<32:16,  5.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 50000\n",
      " Train Loss: 0.2059875875711441\n",
      "Validation Loss: 0.4776913821697235\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 44000/52000 [2:37:49<26:56,  4.95it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 52000\n",
      " Train Loss: 0.2734302282333374\n",
      "Validation Loss: 0.5537917613983154\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 46000/52000 [2:44:58<22:56,  4.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 54000\n",
      " Train Loss: 0.2576228082180023\n",
      "Validation Loss: 0.324387788772583\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 48000/52000 [2:52:12<14:00,  4.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 56000\n",
      " Train Loss: 0.20802640914916992\n",
      "Validation Loss: 0.4687481224536896\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 50000/52000 [2:59:23<07:09,  4.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 58000\n",
      " Train Loss: 0.24838979542255402\n",
      "Validation Loss: 0.44210702180862427\n",
      "Saving checkpoint to run/first_part_review/finetune_lora.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52000/52000 [3:06:39<00:00,  4.64it/s]  \n"
     ]
    }
   ],
   "source": [
    "train_config = TrainConfig(out_dir=\"run/first_part_review\",checkpoint_name=\"finetune_lora.ckpt\",init_from=\"resume\",learning_rate=5e-3)\n",
    "model_config = GPTConfig(block_size=128,use_lora=True,r=8,binary_classification_head=True)\n",
    "rd = reviewsDataset(split=\"train\",max_length=model_config.block_size)\n",
    "train_set, val_set = torch.utils.data.random_split(rd,[0.85,0.15])\n",
    "trainer = Trainer(train_set,val_set,train_config,model_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate using the LoRA finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights for gpt2\n",
      "Number of parameters: 123.65M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varun/Documents/learning/Projects/ml-practice/transformers/sentiment_classification/eval.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.model_config.checkpoint_path,map_location=self.eval_config.device)\n",
      "100%|██████████| 125/125 [00:04<00:00, 27.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model_config = GPTConfig(use_lora=True,binary_classification_head=True,block_size=128,load_from_checkpoint=True,checkpoint_path=\"run/first_part_review/finetune_lora.ckpt\")\n",
    "eval_config = EvalConfig(results_path=\"finetuned_lora.txt\",batch_size=2,subset=True)\n",
    "test_set = reviewsDataset(split=\"test\")\n",
    "evaluator = Eval(test_set=test_set,eval_config=eval_config,model_config=model_config)\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the performance of the LoRA finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics\n",
      "Precision: 0.7698412698412699\n",
      "Recall: 0.776\n",
      "Accuracy: 0.772\n",
      "Metrics by bin\n",
      "| bin          |   TP |   FP |   FN |   TN |   accuracy |   precision |     recall |\n",
      "|:-------------|-----:|-----:|-----:|-----:|-----------:|------------:|-----------:|\n",
      "| (0, 128]     |   20 |    5 |    3 |   10 |   0.789474 |    0.8      |   0.869565 |\n",
      "| (128, 256]   |   47 |   13 |   11 |   54 |   0.808    |    0.783333 |   0.810345 |\n",
      "| (256, 384]   |   16 |    4 |    5 |   15 |   0.775    |    0.8      |   0.761905 |\n",
      "| (384, 512]   |    5 |    5 |    4 |   12 |   0.653846 |    0.5      |   0.555556 |\n",
      "| (512, 640]   |    7 |    0 |    3 |    2 |   0.75     |    1        |   0.7      |\n",
      "| (640, 768]   |    1 |    1 |    1 |    2 |   0.6      |    0.5      |   0.5      |\n",
      "| (768, 896]   |    0 |    1 |    1 |    1 |   0.333333 |    0        |   0        |\n",
      "| (896, 1024]  |    1 |    0 |    0 |    0 |   1        |    1        |   1        |\n",
      "| (1024, 1152] |    0 |    0 |    0 |    0 | nan        |  nan        | nan        |\n",
      "| (1152, 1280] |    0 |    0 |    0 |    0 | nan        |  nan        | nan        |\n",
      "| (1280, 1408] |    0 |    0 |    0 |    0 | nan        |  nan        | nan        |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/qmcgqwhj1mx925j0j_r36xp40000gn/T/ipykernel_32570/3153587618.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  metrics_by_bin = results.groupby('bin').apply(lambda x: pandas.Series({\"TP\": ((x[\"label\"] >= threshold) & (x[\"prediction\"] >= threshold)).sum(),\n",
      "/var/folders/p1/qmcgqwhj1mx925j0j_r36xp40000gn/T/ipykernel_32570/3153587618.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  metrics_by_bin = results.groupby('bin').apply(lambda x: pandas.Series({\"TP\": ((x[\"label\"] >= threshold) & (x[\"prediction\"] >= threshold)).sum(),\n"
     ]
    }
   ],
   "source": [
    "res_file = pandas.read_csv(\"finetuned_lora.txt\")\n",
    "bins = range(0,1500,128)\n",
    "get_metrics_by_bin(res_file,bins,threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
